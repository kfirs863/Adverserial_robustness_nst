{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Image-Classification-using-pre-trained-models.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define helper functions"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T19:38:48.888026Z",
     "start_time": "2024-08-11T19:38:48.867840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from AdaIN_pytorch.utils import adaptive_instance_normalization\n",
    "from AdaIN_pytorch.AdaIN import AdaINNet\n",
    "from pathlib import Path\n",
    "from torch.utils import model_zoo\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'art'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mart\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mattacks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevasion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FastGradientMethod\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mart\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mestimators\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassification\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PyTorchClassifier\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mAdaIN_pytorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m adaptive_instance_normalization\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'art'"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:37:41.213991Z",
     "start_time": "2024-08-11T18:37:41.202933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_path, transform):\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image)\n",
    "    return image\n",
    "\n",
    "# Function to unnormalize and display images\n",
    "def imshow(img, title=None):\n",
    "    img = img.detach()  # Detach the tensor from the computation graph\n",
    "    img = img.cpu()  # Move tensor to CPU\n",
    "    # Reverse the normalization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = img * std + mean  # Unnormalize\n",
    "    img = torch.clamp(img, 0, 1)  # Clip to ensure pixel values are within [0, 1]\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))  # Change from (C, H, W) to (H, W, C)\n",
    "    plt.imshow(npimg)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Function to carry out inference\n",
    "def inference(model, images, classes):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, indices = torch.sort(outputs, descending=True)\n",
    "        percentage = torch.nn.functional.softmax(outputs, dim=1) * 100\n",
    "        results = []\n",
    "        for i in range(images.size(0)):\n",
    "            result = [(classes[idx], percentage[i][idx].item()) for idx in indices[i][:5]]\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "# Function to load class labels\n",
    "def load_classes(file_path):\n",
    "    with open(file_path) as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    return classes\n",
    "\n",
    "# Function to transform images\n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "\n",
    "    ])\n",
    "\n",
    "# Function to create adversarial examples using FGSM\n",
    "def create_adversarial_examples(model, images, epsilon=0.9):\n",
    "    # Get the min and max pixel values of the dataset\n",
    "    min_pixel_value, max_pixel_value = images[0].min(), images[0].max()\n",
    "\n",
    "    # Create PyTorch classifier\n",
    "    classifier = PyTorchClassifier(\n",
    "        model=model,\n",
    "        loss=torch.nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=0.01),\n",
    "        input_shape=(3, 224, 224),\n",
    "        nb_classes=1000,\n",
    "        clip_values=(min_pixel_value, max_pixel_value),\n",
    "        device_type='cpu'\n",
    "    )\n",
    "\n",
    "    # Create FGSM attack\n",
    "    attack = FastGradientMethod(estimator=classifier, eps=epsilon)\n",
    "\n",
    "    # Generate adversarial test examples\n",
    "    adversarial_images = attack.generate(x=images.numpy())\n",
    "    return torch.from_numpy(adversarial_images)\n",
    "\n",
    "# Function to apply style transfer\n",
    "def style_transfer(content_tensor, style_tensor, encoder, decoder, alpha=1.0):\n",
    "    content_enc = encoder(content_tensor)\n",
    "    style_enc = encoder(style_tensor)\n",
    "    transfer_enc = adaptive_instance_normalization(content_enc, style_enc)\n",
    "    mix_enc = alpha * transfer_enc + (1 - alpha) * content_enc\n",
    "    return decoder(mix_enc)\n",
    "\n",
    "# Function to display results as separate tables\n",
    "def display_results_as_tables(results):\n",
    "    # Loop over the results for each image\n",
    "    for i, res in enumerate(results):\n",
    "        # Initialize an empty list to collect rows\n",
    "        rows = []\n",
    "        \n",
    "        for label, score in res:\n",
    "            # Append the result as a dictionary to the list\n",
    "            rows.append({'Label': label, 'Score': f\"{score:.2f}%\"})\n",
    "\n",
    "        # Create DataFrame from the list of rows\n",
    "        df = pd.DataFrame(rows, columns=['Label', 'Score'])\n",
    "\n",
    "        # Display the DataFrame with the image number as the title\n",
    "        print(f\"\\nTop predictions for Image {i+1}:\")\n",
    "        print(df)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load models and images"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:37:43.876639Z",
     "start_time": "2024-08-11T18:37:43.695077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the Weight Transforms\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "transform = weights.transforms()\n",
    "\n",
    "# # Get tranfrom to stack images as tensors\n",
    "# transform = get_transform()\n",
    "\n",
    "# Load and preprocess images\n",
    "dog_img = load_and_preprocess_image('./dog.jpg',transform=transform)\n",
    "# cat_img = load_and_preprocess_image('./cat.jpg', transform)\n",
    "apple_img = load_and_preprocess_image('./green_apple.jpg',transform=transform)\n",
    "\n",
    "x_test = torch.stack([dog_img, apple_img])\n",
    "\n",
    "# Load model and classes\n",
    "resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "classes = load_classes('./imagenet_classes.txt')\n",
    "\n",
    "# Load pretrained model of ResNet on Stylized ImageNet (Resnet_SIN)\n",
    "resnet_sin = models.resnet50(weights=None)\n",
    "url = 'https://bitbucket.org/robert_geirhos/texture-vs-shape-pretrained-models/raw/6f41d2e86fc60566f78de64ecff35cc61eb6436f/resnet50_train_60_epochs-c8e5653e.pth.tar'\n",
    "\n",
    "checkpoint = model_zoo.load_url(url, map_location=torch.device('cpu'))\n",
    "\n",
    "# Remove prefix module. for all keys in checkpoint['state_dict']\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in checkpoint['state_dict'].items():\n",
    "    name = k[7:]  # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "# Load the new state dict\n",
    "resnet_sin.load_state_dict(new_state_dict)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNet50_Weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Initialize the Weight Transforms\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m weights \u001B[38;5;241m=\u001B[39m \u001B[43mResNet50_Weights\u001B[49m\u001B[38;5;241m.\u001B[39mDEFAULT\n\u001B[1;32m      3\u001B[0m transform \u001B[38;5;241m=\u001B[39m weights\u001B[38;5;241m.\u001B[39mtransforms()\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# # Get tranfrom to stack images as tensors\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# transform = get_transform()\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Load and preprocess images\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ResNet50_Weights' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Display original images and perform inference on both ResNet and ResNet_SIN models"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "imshow(x_test[0], title='Original Dog')\n",
    "imshow(x_test[1], title='Original Green Apple')\n",
    "\n",
    "# Display images in a grid\n",
    "imshow(make_grid(x_test), title=['Dog (Original)', 'Green Apple (Original)'])\n",
    "\n",
    "\n",
    "# Perform inference on original images\n",
    "print(\"Original images inference on Resnet50:\")\n",
    "results = inference(resnet, x_test, classes)\n",
    "display_results_as_tables(results)\n",
    "     \n",
    "# Perform inference on original images using the stylized model\n",
    "print(\"\\nOriginal images inference on Resnet50_SIN:\")\n",
    "results_sin = inference(resnet_sin, x_test, classes)\n",
    "display_results_as_tables(results_sin)\n",
    "\n",
    "   "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create adversarial examples using FGSM on ResNet and perform inference"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define epsilon values\n",
    "epsilon = 8/256\n",
    "\n",
    "# Create adversarial examples for original ResNet model\n",
    "x_test_adv_resnet = create_adversarial_examples(resnet, x_test, epsilon=epsilon)\n",
    "\n",
    "# Display adversarial examples\n",
    "imshow(make_grid(x_test_adv_resnet), title=['Dog (ResNet Adv)', 'Green Apple (ResNet Adv)', f'{epsilon=}'])\n",
    "\n",
    "# # # Save x_test_adv_resnet as image\n",
    "# x_test_adv_resnet = x_test_adv_resnet.permute(0, 2, 3, 1).numpy()\n",
    "# x_test_adv_resnet = (x_test_adv_resnet).astype(np.uint8)\n",
    "# Image.fromarray(x_test_adv_resnet[0]).save('dog_adv.jpg')\n",
    "# Image.fromarray(x_test_adv_resnet[1]).save('green_apple_adv.jpg')\n",
    "# # \n",
    "# # # Load images back to x_test_adv_resnet\n",
    "# x_test_adv_resnet = torch.stack([read_image('dog_adv.jpg'), read_image('green_apple_adv.jpg')]).to(torch.float32)\n",
    "\n",
    "\n",
    "# Perform inference on adversarial examples using the original model\n",
    "print(f\"Adversarial images inference using original resnet with {epsilon=}:\")\n",
    "results_adv = inference(resnet, x_test_adv_resnet, classes)\n",
    "display_results_as_tables(results_adv)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create adversarial examples using FGSM on ResNet_SIN and perform inference"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create adversarial examples for ResNet_SIN model\n",
    "x_test_adv_resnet_sin = create_adversarial_examples(resnet_sin, x_test, epsilon=epsilon)\n",
    "\n",
    "# Display adversarial examples\n",
    "imshow(make_grid(x_test_adv_resnet_sin), title=['Dog (ResNet_SIN Adv)', 'Green Apple  (ResNet_SIN Adv)', f'{epsilon=}'])\n",
    "        \n",
    "# Perform inference on adversarial examples using the stylized model\n",
    "print(f\"\\nAdversarial images inference using resnet_SIN with {epsilon=}:\")\n",
    "results_adv_stylized = inference(resnet_sin, x_test_adv_resnet_sin, classes)\n",
    "display_results_as_tables(results_adv_stylized)\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load AdaIN model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load AdaIN model\n",
    "vgg = torch.load('./AdaIN_pytorch/vgg_normalized.pth')\n",
    "model = AdaINNet(vgg).to('cpu')  # Assuming CPU for demonstration\n",
    "model.decoder.load_state_dict(torch.load('./AdaIN_pytorch/decoder.pth'))\n",
    "model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configure style transfer and alpha value"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select and load style image\n",
    "style_image = Path('AdaIN_pytorch/images/texture/paper_texture.jpg')\n",
    "style_img = load_and_preprocess_image(style_image, transform)\n",
    "\n",
    "# Define alpha value\n",
    "alpha = 0.1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Perform style transfer on adversarial examples from ResNet and ResNet_SIN models"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perform style transfer on adversarial examples from ResNet model\n",
    "x_test_adv_resnet_stylized = [style_transfer(img.unsqueeze(0), style_img.unsqueeze(0), model.encoder, model.decoder,alpha=alpha) for img in x_test_adv_resnet]\n",
    "x_test_adv_resnet_stylized = torch.cat(x_test_adv_resnet_stylized)\n",
    "\n",
    "# Display stylized adversarial examples\n",
    "imshow(make_grid(x_test_adv_resnet_stylized), title=['Dog (Resnet Stylized Adv)', 'Green Apple  (Resnet Stylized Adv)',f'{alpha=}'])\n",
    "\n",
    "# Perform inference on stylized adversarial examples from ResNet_SIN model\n",
    "x_test_adv_resnet_sin_stylized = [style_transfer(img.unsqueeze(0), style_img.unsqueeze(0), model.encoder, model.decoder,alpha=alpha) for img in x_test_adv_resnet_sin]\n",
    "\n",
    "# Display stylized adversarial examples\n",
    "x_test_adv_resnet_sin_stylized = torch.cat(x_test_adv_resnet_sin_stylized)\n",
    "\n",
    "# Display stylized adversarial examples\n",
    "imshow(make_grid(x_test_adv_resnet_sin_stylized), title=['Dog (Resnet_SIN Stylized Adv)', 'Green Apple  (Resnet_SIN Stylized Adv)',f'{alpha=}'])\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Perform inference on stylized adversarial examples"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Perform inference on stylized adversarial examples using ResNet model\n",
    "print(\"Stylized adversarial ResNet images inference:\")\n",
    "print('--------------------------------------------')\n",
    "results_stylized_adv = inference(resnet, x_test_adv_resnet_stylized, classes)\n",
    "display_results_as_tables(results_stylized_adv)\n",
    "\n",
    "\n",
    "# Perform inference on stylized adversarial examples using ResNet_SIN model\n",
    "print(\"\\nStylized adversarial ResNet_SIN images inference:\")\n",
    "print('--------------------------------------------')\n",
    "results_stylized_adv = inference(resnet_sin, x_test_adv_resnet_sin_stylized, classes)\n",
    "display_results_as_tables(results_stylized_adv)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Apply style transfer on original images"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to create a grid of images\n",
    "def create_image_grid(tensor, nrow=1, padding=2):\n",
    "    # Check if the input is a list of tensors or a single batched tensor\n",
    "    if isinstance(tensor, list):\n",
    "        tensor = torch.stack(tensor)\n",
    "\n",
    "    # Determine the size of each image\n",
    "    n, c, h, w = tensor.shape\n",
    "    ncol = (n + nrow - 1) // nrow\n",
    "\n",
    "    # Create a blank canvas for the grid\n",
    "    grid_height = nrow * h + (nrow - 1) * padding\n",
    "    grid_width = ncol * w + (ncol - 1) * padding\n",
    "    grid = torch.full((c, grid_height, grid_width), 1.0)  # Using 1.0 to create white padding\n",
    "\n",
    "    # Place each image onto the canvas\n",
    "    for idx in range(n):\n",
    "        row = idx // ncol\n",
    "        col = idx % ncol\n",
    "        top = row * (h + padding)\n",
    "        left = col * (w + padding)\n",
    "        grid[:, top:top+h, left:left+w] = tensor[idx]\n",
    "\n",
    "    return grid"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x_test_stylized = [style_transfer(img.unsqueeze(0), style_img.unsqueeze(0), model.encoder, model.decoder,alpha=alpha) for img in x_test]\n",
    "x_test_stylized = torch.cat(x_test_stylized)\n",
    "print(x_test_stylized.shape)\n",
    "# Display stylized images\n",
    "imshow(create_image_grid(x_test_stylized), title=['Dog (Resnet Stylized)', 'Green Apple  (Resnet Stylized)',f'{alpha=}'])\n",
    "\n",
    "# Perform inference on stylized images using ResNet model\n",
    "print(\"Stylized ResNet images inference:\")\n",
    "print('--------------------------------------------')\n",
    "results_stylized = inference(resnet, x_test_stylized, classes)\n",
    "display_results_as_tables(results_stylized)\n",
    "\n",
    "# Perform inference on stylized images using ResNet_SIN model\n",
    "print(\"\\nStylized ResNet_SIN images inference:\")\n",
    "print('--------------------------------------------')\n",
    "results_stylized = inference(resnet_sin, x_test_stylized, classes)\n",
    "display_results_as_tables(results_stylized)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load image without adversarial examples from path\n",
    "path_no_adv = '/mobileye/RPT/users/kfirs/kfir_project/non_adversarials_subset/binoculars/image_331.jpg'\n",
    "path_adv = '/mobileye/RPT/users/kfirs/kfir_project/adversarials_subset/binoculars/image_331.npy'\n",
    "\n",
    "# Load and preprocess images\n",
    "no_adv_img = load_and_preprocess_image(path_no_adv,transform=transform)\n",
    "\n",
    "# Load npy file to PIL image\n",
    "adv_image = np.load(path_adv).squeeze().astype(np.uint8)\n",
    "adv_image = Image.fromarray(adv_image)\n",
    "\n",
    "adv_image_tensor = transform(adv_image)\n",
    "\n",
    "x_test_no_adv = torch.stack([no_adv_img, adv_image_tensor])\n",
    "\n",
    "# Perform inference on original images\n",
    "print(\"Original images inference on Resnet50:\")\n",
    "results = inference(resnet, x_test_no_adv, classes)\n",
    "display_results_as_tables(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
